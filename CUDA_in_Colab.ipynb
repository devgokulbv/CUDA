{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devgokulbv/CUDA/blob/main/CUDA_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install CUDA C++ plugin for Colab:\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntOgKF_-y0SV",
        "outputId": "d47e6fb8-302b-4e46-a500-65e84b4f5d1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpu3vo327a\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect selected GPU and its NVIDA architecture:\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits\")\n",
        "if \"not found\" in gpu_info.lower(): raise RuntimeError(\"Error: No GPU found. Please select a GPU runtime environment.\")\n",
        "gpu_name, compute_cap = map(str.strip, gpu_info.split(','))\n",
        "gpu_arch = f\"sm_{compute_cap.replace('.', '')}\"\n",
        "\n",
        "print(f\"{'GPU Name':<15}: {gpu_name}\")\n",
        "print(f\"{'Architecture':<15}: {gpu_arch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqh0baiO15d",
        "outputId": "b6c4ef2d-53e4-4392-be46-d9fc6742d6fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name       : Tesla T4\n",
            "Architecture   : sm_75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello_kernel() {\n",
        "    int blockId = blockIdx.x;\n",
        "    int threadId = threadIdx.x;\n",
        "    int globalId = threadId + blockId * blockDim.x;\n",
        "\n",
        "    printf(\"Hello from block %d, thread %d (global thread %d)\\n\", blockId, threadId, globalId);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int numBlocks = 2;\n",
        "    int threadsPerBlock = 4;\n",
        "\n",
        "    hello_kernel<<<numBlocks, threadsPerBlock>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HW8N8RXzCTc",
        "outputId": "b7bcabf9-fbad-4bfd-dec6-9121031ecb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block 0, thread 0 (global thread 0)\n",
            "Hello from block 0, thread 1 (global thread 1)\n",
            "Hello from block 0, thread 2 (global thread 2)\n",
            "Hello from block 0, thread 3 (global thread 3)\n",
            "Hello from block 1, thread 0 (global thread 4)\n",
            "Hello from block 1, thread 1 (global thread 5)\n",
            "Hello from block 1, thread 2 (global thread 6)\n",
            "Hello from block 1, thread 3 (global thread 7)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#define row 3\n",
        "#define col 3\n",
        "\n",
        "__global__ void kernel(float *c,float *a, float *b, int n)\n",
        "{\n",
        "    int global_id = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (global_id < n)\n",
        "    {\n",
        "        c[global_id] = a[global_id] + b[global_id];\n",
        "    }\n",
        "}\n",
        "int main()\n",
        "{\n",
        "    float a[row][col], b[row][col], c[row][col];\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    int size = row * col * sizeof(float);\n",
        "    int num = row * col;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "\n",
        "    for (int i = 0; i < row; i++)\n",
        "    {\n",
        "        for (int j = 0; j < col; j++)\n",
        "        {\n",
        "            a[i][j] = rand() % 100;\n",
        "            b[i][j] = rand() % 100;\n",
        "            printf(\"%f \", a[i][j]);\n",
        "\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    for (int i = 0; i < row; i++)\n",
        "    {\n",
        "        for (int j = 0; j < col; j++)\n",
        "        {\n",
        "\n",
        "            printf(\"%f \", b[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    kernel<<<3, 3>>>(d_c, d_a, d_b, num);\n",
        "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int i = 0; i < row; i++)\n",
        "    {\n",
        "        for (int j = 0; j < col; j++)\n",
        "        {\n",
        "            printf(\"%f \", c[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc0x6Wv57HeA",
        "outputId": "4789fb22-e908-4b2d-87e8-c2e8ac8e5dcd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83.000000 77.000000 93.000000 \n",
            "86.000000 49.000000 62.000000 \n",
            "90.000000 63.000000 40.000000 \n",
            "\n",
            "86.000000 15.000000 35.000000 \n",
            "92.000000 21.000000 27.000000 \n",
            "59.000000 26.000000 26.000000 \n",
            "\n",
            "169.000000 92.000000 128.000000 \n",
            "178.000000 70.000000 89.000000 \n",
            "149.000000 89.000000 66.000000 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "\n",
        "#define N 512  // Matrix size N x N\n",
        "\n",
        "__global__ void matrixMulKernel(float* A, float* B, float* C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;  // Row index\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;  // Column index\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            sum += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocate host memory\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialize matrices\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        h_A[i] = 1.0f;  // Example values\n",
        "        h_B[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((N + 15) / 16, (N + 15) / 16);\n",
        "    matrixMulKernel<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    free(h_A); free(h_B); free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "D6zN7pMCXEXw",
        "outputId": "37da68a0-2e30-4412-a052-2910bbbaae57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%cuda` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "#define N 4\n",
        "#define BLOCK_WIDTH 2\n",
        "\n",
        "__global__ void matrixMulKernel(int *a, int *b, int *c)\n",
        "{\n",
        "    int row = blockIdx.y *blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int sum = 0;\n",
        "    if (row < N && col < N)\n",
        "    {\n",
        "        for (int i = 0; i < N; i++)\n",
        "        {\n",
        "            sum += a[row * N + i] * b[i * N + col];\n",
        "        }\n",
        "        c[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int matrixA[N][N], matrixB[N][N], matrixC[N][N];\n",
        "    int *d_matrixA, *d_matrixB, *d_matrixC;\n",
        "\n",
        "    for(int i = 0; i < N; i++)\n",
        "    {\n",
        "        for(int j = 0; j < N; j++)\n",
        "        {\n",
        "            matrixA[i][j] = 1;\n",
        "            matrixB[i][j] = 2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMalloc(&d_matrixA, N * N * sizeof(int));\n",
        "    cudaMalloc(&d_matrixB, N * N * sizeof(int));\n",
        "    cudaMalloc(&d_matrixC, N * N * sizeof(int));\n",
        "    cudaMemcpy(d_matrixA, matrixA, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_matrixB, matrixB, N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 threadsPerBlock(BLOCK_WIDTH, BLOCK_WIDTH);\n",
        "    dim3 blocksPerGrid((N + BLOCK_WIDTH - 1) / BLOCK_WIDTH, (N + BLOCK_WIDTH - 1) / BLOCK_WIDTH);\n",
        "    matrixMulKernel<<<blocksPerGrid, threadsPerBlock>>>(d_matrixA, d_matrixB, d_matrixC);\n",
        "\n",
        "\n",
        "    cudaMemcpy(matrixC, d_matrixC, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    for(int i = 0; i < N; i++)\n",
        "    {\n",
        "        for(int j = 0; j < N; j++)\n",
        "        {\n",
        "            printf(\"%d \", matrixA[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    for(int i = 0; i < N; i++)\n",
        "    {\n",
        "        for(int j = 0; j < N; j++)\n",
        "        {\n",
        "            printf(\"%d \", matrixB[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    for(int i = 0; i < N; i++)\n",
        "    {\n",
        "        for(int j = 0; j < N; j++)\n",
        "        {\n",
        "            printf(\"%d \", matrixC[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "jiQ-nIKNXJY9",
        "outputId": "be7d2eca-aabf-4da8-f183-273af1ef0b4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1 1 \n",
            "1 1 1 1 \n",
            "1 1 1 1 \n",
            "1 1 1 1 \n",
            "\n",
            "2 2 2 2 \n",
            "2 2 2 2 \n",
            "2 2 2 2 \n",
            "2 2 2 2 \n",
            "\n",
            "8 8 8 8 \n",
            "8 8 8 8 \n",
            "8 8 8 8 \n",
            "8 8 8 8 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}